{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub  # Not needed once model is downloaded/saved\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# In the example code a non-existing tokenizer is used. I choose the squad one which is also for Q&A\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "new_model = tf.keras.models.load_model('model/1')  # New directory (also not really needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile()  # for local inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code from tf hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(1, 64), dtype=float32, numpy=\n",
      "array([[ 7.5831766, -7.126385 , -8.315767 , -8.040369 , -7.9417977,\n",
      "        -8.029746 , -7.820639 , -8.331517 , -8.144661 , -9.128356 ,\n",
      "        -8.238922 , -6.824143 , -6.851987 , -8.14085  , -7.275723 ,\n",
      "        -5.711597 , -6.488315 , -8.637387 , -8.109914 , -4.772206 ,\n",
      "        -8.957758 , -8.470302 , -6.1216087, -8.032608 , -7.8976817,\n",
      "        -8.5412445, -6.8471313, -4.765982 , -7.4818387, -6.6101646,\n",
      "         1.5490808,  4.649346 , -4.1442738, -3.5947165, -7.098928 ,\n",
      "        -3.7559516, -6.139193 , -6.8271527, -6.8225207, -8.005595 ,\n",
      "        -8.341232 , -7.9348207, -8.764536 , -8.705311 , -9.039872 ,\n",
      "        -7.3882203, -8.58604  , -8.926344 , -6.628455 , -8.407904 ,\n",
      "        -6.4927077, -8.317963 , -8.328851 , -7.268306 , -4.6447043,\n",
      "        -6.8083863, -6.0284085, -1.6590921, -8.3523   , -8.886298 ,\n",
      "        -8.954616 , -8.860009 , -9.228969 , -7.570626 ]], dtype=float32)>, <tf.Tensor: shape=(1, 64), dtype=float32, numpy=\n",
      "array([[ 7.669568 , -4.8944187, -5.455489 , -6.0652227, -7.191206 ,\n",
      "        -6.3862805, -6.6618443, -7.588853 , -7.8051934, -7.023631 ,\n",
      "        -5.673487 , -5.051487 , -7.7765837, -7.760078 , -5.3412623,\n",
      "        -7.8842783, -6.477276 , -7.4216766, -7.993879 , -8.040374 ,\n",
      "        -4.3626423, -5.971758 , -8.570064 , -8.288323 , -8.304952 ,\n",
      "        -6.284233 , -8.182134 , -8.400278 , -8.154895 , -5.143589 ,\n",
      "        -5.8217893, -2.0809882, -1.4842976,  3.7917194, -3.3755891,\n",
      "         3.799563 , -4.6850467, -6.8912177, -7.438336 , -3.451262 ,\n",
      "        -2.1997201, -7.898609 , -8.015927 , -8.113152 , -6.609105 ,\n",
      "        -8.75466  , -8.448581 , -7.267312 , -8.158035 , -6.817866 ,\n",
      "        -8.071009 , -7.7153945, -6.099531 , -7.618623 , -8.192704 ,\n",
      "        -7.341666 , -7.991354 , -1.4982309, -2.54897  , -7.1659145,\n",
      "        -7.9836683, -7.9212856, -7.3539762, -7.771076 ]], dtype=float32)>, <tf.Tensor: shape=(1, 64), dtype=float32, numpy=\n",
      "array([[ 7.0741615, -5.491906 , -6.964512 , -7.3522096, -6.8774276,\n",
      "        -6.9747057, -7.2705445, -7.841131 , -8.291459 , -7.7393165,\n",
      "        -7.620135 , -3.5219855, -7.530895 , -7.94638  , -5.543569 ,\n",
      "        -8.823166 , -9.0406475, -8.796883 , -8.780493 , -8.7643385,\n",
      "        -8.562357 , -8.357903 , -7.9660273, -8.314984 , -8.014567 ,\n",
      "        -8.024875 , -8.177991 , -7.0992627, -7.258577 , -2.3997207,\n",
      "        -4.463882 , -2.697795 , -5.432605 , -4.5746408, -8.187237 ,\n",
      "        -5.0986934, -7.5805116, -8.412291 , -8.75995  , -7.8933578,\n",
      "        -8.330961 , -8.480795 , -8.479025 , -8.247843 , -8.331284 ,\n",
      "        -8.45396  , -8.062819 , -8.205042 , -8.580544 , -9.075197 ,\n",
      "        -8.818941 , -9.009725 , -8.966894 , -9.244474 , -8.327902 ,\n",
      "        -8.489567 , -8.796795 , -5.475051 , -8.433924 , -8.694561 ,\n",
      "        -8.505011 , -8.35059  , -8.215558 , -8.790848 ]], dtype=float32)>]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8760e4baebe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0;31m# using `[1:]` will enforce an answer. `outputs[0][0][0]` is the ignored '[CLS]' token logit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mshort_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    'How long did it take to find the answer?',\n",
    "    'What\\'s the answer to the great question?',\n",
    "    'What\\'s the name of the computer?']\n",
    "paragraph = '''<p>The computer is named Deep Thought.</p>.\n",
    "               <p>After 46 million years of training it found the answer.</p>\n",
    "               <p>However, nobody was amazed. The answer was 42.</p>'''\n",
    "\n",
    "for question in questions:\n",
    "  question_tokens = tokenizer.tokenize(question)\n",
    "  paragraph_tokens = tokenizer.tokenize(paragraph)\n",
    "  tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + paragraph_tokens + ['[SEP]']\n",
    "  input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  input_mask = [1] * len(input_word_ids)\n",
    "  input_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(paragraph_tokens) + 1)\n",
    "\n",
    "  input_word_ids, input_mask, input_type_ids = map(lambda t: tf.expand_dims(\n",
    "      tf.convert_to_tensor(t, dtype=tf.int32), 0), (input_word_ids, input_mask, input_type_ids))\n",
    "  outputs = new_model([input_word_ids, input_mask, input_type_ids])\n",
    "    \n",
    "  print(outputs)\n",
    "  print(outputs.shape)\n",
    "  # using `[1:]` will enforce an answer. `outputs[0][0][0]` is the ignored '[CLS]' token logit\n",
    "  short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
    "  short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
    "  answer_tokens = tokens[short_start: short_end + 1]\n",
    "  answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "  print(f'Question: {question}')\n",
    "  print(f'Answer: {answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['How long did it take to find the answer?']\n",
    "paragraph = '''<p>The computer is named Deep Thought.</p>.\n",
    "               <p>After 46 million years of training it found the answer.</p>\n",
    "               <p>However, nobody was amazed. The answer was 42.</p>'''\n",
    "\n",
    "question_tokens = tokenizer.tokenize(question)\n",
    "paragraph_tokens = tokenizer.tokenize(paragraph)\n",
    "tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + paragraph_tokens + ['[SEP]']\n",
    "input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_mask = [1] * len(input_word_ids)\n",
    "input_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(paragraph_tokens) + 1)\n",
    "\n",
    "input_word_ids, input_mask, input_type_ids = map(lambda t: tf.expand_dims(\n",
    "  tf.convert_to_tensor(t, dtype=tf.int32), 0), (input_word_ids, input_mask, input_type_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": {    \n",
    "        \"input_word_ids\": input_word_ids.numpy().tolist(),\n",
    "        \"input_mask\": input_mask.numpy().tolist(),\n",
    "        \"input_type_ids\": input_type_ids.numpy().tolist()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_serving_url = \"http://localhost:8501/v1/models/bert_qa:predict\"\n",
    "r = requests.post(tf_serving_url, data=json.dumps(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = r.json()[\"outputs\"][\"tf_bert_for_natural_question_answering_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: deep thought\n"
     ]
    }
   ],
   "source": [
    "short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
    "short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
    "answer_tokens = tokens[short_start: short_end + 1]\n",
    "answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "print(f'Answer: {answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
